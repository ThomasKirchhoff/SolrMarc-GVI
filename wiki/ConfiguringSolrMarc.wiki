#summary This page will guide you through configuring and using SolrMarc
<wiki:toc max_depth="5" />

<br />
= About This Guide =

This configuring SolrMarc guide explains the setup of the two configuration files that control the operation of SolrMarc and gives examples of custom indexing routines.

<br />

= About !SolrMarc =

A Configurable Java-based program for indexing MARC records into a Solr index.

The !SolrMarc program reads in MARC records stored in standard binary format (ISO 2709) and uses a configurable and customizable script for extracting values from the fields and sub-fields of the MARC record to build an index entry for adding to Solr.  Additionally, so that it can run faster, rather than building the records and POSTing them to a Solr search engine, this program directly writes to the index directory of the Solr search engine.

<br />

= config.properties File =

The main configuration of the program is currently done via a properties file that is created and stored in the jar file produced by the build process. The name of this file can be passed in as the first parameter on the command line, however you can declare one particular config.properties file to be the default.  Then if a command doesn't specify which config.properties file to use, that default one will be used.  An example config.properties file is below (more details about the entries follow):

{{{
# Properties for the !SolrMarc import program
#  for more documentation, see 
#  http://code.google.com/p/solrmarc/wiki/ConfiguringSolrMarc

# solrmarc.solr.war.path - must point to either a war file for the version of Solr 
# that you want to use, or to a directory of jar files extracted from a Solr war 
# files.  If this is not provided, SolrMarc can only work by communicating with 
# a running Solr server.
solrmarc.solr.war.path=jetty/webapps/solr.war

# solrmarc.custom.jar.path - Jar containing custom java code to use in indexing. 
# If solr.indexer below is defined (other than the default of org.solrmarc.index.SolrIndexer)
# you MUST define this value to be the Jar containing the class listed there. 
solrmarc.custom.jar.path=

# - solr.indexer.properties - indicates how to populate Solr index fields from
#   marc data.  This is the core configuration file for solrmarc.
solr.indexer.properties = demo_index.properties, demo_local_index.properties

# - solr.indexer - full name of java class with custom indexing functions. This 
#   class must extend org.solrmarc.index.SolrIndexer, which is default
solr.indexer = org.solrmarc.index.SolrIndexer

# - marc_permissive - if true, try to recover from errors, including records
#  with errors, when possible
marc_permissive = true

# --- Solr instance properties -------------------------------------------------
# - solr.path - path to your Solr instance
solr.path = jetty/solr

# - solr.data.dir - path to data directory for your Solr instance 
#   (note: Solr can be configured to use a different data directory)
solr.data.dir = jetty/solr/data

# - solr.hosturl - optional URL to send commit to Solr after indexing to avoid
#   need for Solr restart.  Expects an XmlUpdateRequestHandler;  see
#  http://wiki.apache.org/solr/UpdateXmlMessages
solr.hosturl = http://localhost:8983/solr/update


# -- MARC data properties ------------------------------------------------------

# - marc.default_encoding - possible values are MARC8, UTF-8, UNIMARC, BESTGUESS
marc.default_encoding = MARC8

# - marc.to_utf_8 - if true, this will convert records in our import file from 
#   MARC8 encoding into UTF-8 encoding on output to index
marc.to_utf_8 = true

# - marc.include_erros - when error in marc record, dump description of error 
#   to field in solr index an alternative way to trap the indexing error 
#   messages that are logged during index time.  Nice for staff b/c they can 
#   search for errors and see ckey and record fields in discovery portal.  This 
#   field is NOT used for other queries.  Solr schema.xml must have field 
#   marc_error.
marc.include_errors = false

}}}

<br />

=== config.properties File Entries ===
 * *solrmarc.solr.war.path: * Must point to either a war file for the version of Solr that you want to use, or to a directory of jar files extracted from a Solr war file.  If this is not provided, SolrMarc can only work by communicating over HTTP with a running Solr server.

 * *solrmarc.custom.jar.path: * Name of Jar containing custom java code to use in indexing. If solr.indexer below is defined (other than the default value of !org.solrmarc.index.SolrIndexer) you MUST define this value to be the Jar containing the class listed there. 

 * *solr.path:* lists the full path to the Solr home directory, which should contain the directories {{{etc}}}, {{{conf}}} and {{{data}}}.  The directory {{{conf}}} should contain a file named _schema.xml_ which describes the different indexing fields that the instance of the Solr search engine is expecting, and the data directory is where the Solr index files will be placed. Note that if the special value of {{{REMOTE}}} is used here, SolrMarc will use the value for solr.hosturl (described below) and send updates directly to that Solr server over a HTTP connection.

 * *solr.data.dir: * Path to data directory for your Solr instance. Note that if this property is not defined, a default value of ${solr.path}/data will be used.  Note also that if you are pointing at a multi-core solr configuration this value will likely be ignored, and the the data directory will be defined relative to the core specified in the property solr.core.name

 * *solr.core.name: * (optional) Name of solr core to use when you are pointing at a multi-core solr configuration.  This property should only be used when you are working with a multi-core solr configuration.

 * *solr.indexer.properties:* lists the name of the properties file that defines the mapping from the MARC record fields and sub-fields to the Solr index entries. For details of how this mapping is done, please see below.  Note that multiple property files can be listed here separated by commas. Index specification entries found in the second (or subesequent) file, either add to the Index specification of the first file, or override entries found there.
 
 * *solr.indexer:* (optional); specifies the name of a custom indexing class which defines any custom routines for extracting and mapping data for a particular Solr index field. These custom routines will only be needed in cases where values from several different MARC fields and sub-fields need to be consulted to determine the value to add to the Solr index field. Several examples of these custom indexing routines are given below. If, however, you can handle all of the necessary MARC field-to-Solr index field mappings using the available specification language, you do not need to define a value for this entry, and the default !SolrIndexer class will be used.

 * *solr.hosturl:* (optional); Setting this value means a "commit" message is sent to a running Solr URL when the indexing process completes.  If Solr is running while its index is being updated, it doesn't automatically know about changes to the index:  either Solr will need to be restarted to find the new data, or solr.hosturl can be set so that a "commit" message is sent to Solr to tell it to reload the index.  The solr.hosturl value should be the URL of an XmlUpdateRequestHandler for the Solr instance, generally http://yer_solr_host:port/solr/update.  To restate:  if this value is given, when the indexing process completes, SolrMarc will send a signal to the currently running Solr search engine, which will cause it to read the newly modified index files. This is primarily a convenience function, to avoid having to stop and restart the running Solr search engine. If no search engine is running at the URL specified, the program will quietly proceed along its way, with the understanding that when that search engine is eventually restarted it will read in the new index data anyway.

 * *marc.to_utf_8:* a boolean value, defaulting to false.  If it is set to true, then as MARC records are read in the program translate the fields of the record to UTF-8, prior to sending the record to the indexer.  If the records are determined to already be encoded in UTF-8 the field values will be unchanged. <br><br><b>Note:</b>  You are strongly recommended to use this setting to translate your records to UTF-8, any of the other character encodings that are commonly used in MARC records likely to cause display problems for special characters that occur in the data.

 * *marc.permissive:* a boolean value, defaulting to false.  If it is set to true, then as MARC records are read in, if the program encounters ill-formed records, it will make an attempt to work around the problem and read in and index the record rather than simply skipping over the bad record.  Note that the feature does not and cannot fix records that have a valid MARC record structure, but which have incorrect or invalid data entered in some fields. 

 * *marc.default_encoding:* valid values:  {{{MARC8}}}, {{{UTF8}}}, {{{UNIMARC}}}, {{{ISO8859_1}}}, or {{{BESTGUESS}}}.  It is used to specify what encoding is used in the marc records that you will be importing.  In all likelihood your data will be in one of the first two listed above, or maybe in the third if you are a European institution.  If you have no idea what encoding your original data is in, you can specify {{{BESTGUESS}}} and the program will do its best to correctly determine what encoding is used.  Note also if you provide an incorrect value for this entry, _and_ if you set the entry {{{marc.permissive}}} to true, the MARC record reader will try to detect when you are wrong, and do its best to recover from the problem.

 * *marc.include_errors:* a boolean value, defaulting to false.  If it is set, _and_ if {{{marc.permissive}}} is also set to true, then any errors that are encountered in the process of reading a MARC record, will be stored in the solr index record that is produced, this will allow catalogers to review the errors encountered during reading and to correct the original records in which the errors occurred.

 * *marc.unicode_normalize_setting:* (added in SolrMarc 2.1.1)  If it is set, Unicode characters found in the record will be normalized using the specified method: {{{C}}} (Canonical decomposition followed by canonical composition), {{{D}}} (Canonical decomposition), {{{KC}}} (Compatibility decomposition followed by canonical composition) or {{{KD}}} (Compatibility decomposition).  More information on Unicode normalization can be found in [http://unicode.org/reports/tr15/ this technical report].  Some helpful commentary on the use of this setting can be found in [http://bibwild.wordpress.com/2010/05/13/unicode-normalization-forms/ Jonathan Rochkind's blog].

<br />

=== Special Purpose config.properties File Entries ===

Lastly, there are several special purpose entries that will not be used in general, and usually won’t appear in the config.properties file.

 * *marc.override:* lists the class name of the implementation of the MarcFactory object to use to override the default one provided in the {{{marc4j}}} library.  You almost certainly will not need to use this entry.  It is needed at UVa since the MARC records that we are working from have multiple 001 fields (which actually makes them invalid MARC records) and rather than simply keeping the last of these 001 fields that occurs (and discarding the rest), we need to specifically select the correct 001 field to use in the indexing process.  Another option for this field is if you want to ensure that the fields in the MARC record are not sorted into ascending order as they are read in (which marc4j does by default) you can override this behavior by defining this property to the value {{{org.solrmarc.marcoverride.UVAMarcFactoryImpl}}} 

 * *solr.optimize_at_end:* a boolean value, defaulting to false. If it is set to true, when the program has finished indexing all of the MARC records provided to it, it will optimize the index, which will make searches perform much faster.  This entry is set by the shell script {{{optimizesolr}}}, which is found in the dist directory.

 * *marc.ids_to_delete:* supplies the name of a file that contains a list of Solr item ids, one per line, that are to be removed from the Solr index.

 * *marc.delete_record_id_mapper:* used in conjunction with {{{marc.ids_to_delete}}} when the entries that occur in the {{{marc.ids_to_delete}}} file need to be processed to correspond to the actual ids that occur in the solr index. For instance at UVa the records have ids like _u184783_ but the entry that would appear in the {{{marc.ids_to_delete}}} file produced by our system would be merely 184783.

 * *marc.verbose:* a boolean value. If it is set to true, instructs the program to print out the entire MARC record as it is read in, followed by the entire index record that will be sent to Solr.

 * *marc.include_if_present* and *marc.include_if_missing:* used for reprocessing records that have already been indexed, if a new index field is added.  They are only useful when the newly added field draws its information from fields that only occur in a small subset of records. For example, in the Indexer configuration file shown below, the following field specification:<br><br>   {{{instrument_facet = 048m[0-1], instrument_map}}}<br><br> specifies that the information should be found in the 048m field of the MARC record. This was a typo, it should have read 048a, therefore no index records were created with a value for {{{instrument_facet}}}.  To fix it, rather than re-index all of our records, I was able to set {{{marc.include_if_present = 048a}}}  which specifies that only those records that have one or more 048a fields will be re-indexed, the rest will be skipped (which is fine since the indexed record would be the same in any case.)<br>*Note:* Rather than setting these entries, you should use the {{{filterrecords}}} script which internally uses these settings to operate as a marc-based-grep program.

<br />

= index.properties File =

The specification of which MARC fields and subfields are to be mapped to which Solr index fields is handled via an index.properties configuration file, the exact name of which is specified by the {{{solr.indexer.properties}}} entry in the config.properties file as described above. Since the field specification configuration file is a properties file, there are certain constraints of how the file is structured. Basically all properties files consist of a number of pairs of values separated by an equals sign. The key values represent the name of the field that will be added to the Solr document. All of these must either match a field definition that occurs in the _schema.xml_ file for the Solr search engine, or they must match a !dynamicField definition in that file.   

If any Solr field entries are listed here that do not match either a field definition or a !dynamicField definition from the _schema.xml_ file, the indexing will fail.

Other indexing errors can occur that will not prevent the creation of a Solr index.  For example, if a MARC record cannot be read due to a munged leader field, an error message is printed and the record is skipped.  Another example:  a Solr field is not defined as !multiValued in the Solr schema.xml file, but multiple values are created by !SolrMarc (e.g. because the MARC record has multiple 245 fields).

 {{{
id = 001, first
author_text = 100a:110a:111a:130a
author_display = 100a:110a
published_text = 260a
material_type_text = 300a
notes_text = 500a:505a
uniform_title_text = 240a:240b
uniform_title_display = 240a
uniform_subtitle_display = 240b
marc_display = FullRecordAsXML
marc_text = custom, getAllSearchableFields(100, 900)

title_text = 245a:245b:240a:240b:700t 
title_display = 245a
subtitle_display = 245b 
title_added_entry_display = 700t
call_number_text = custom, getCallNumberCleaned
call_number_display = 999a:090a:050a, first
year_multisort_i = DateOfPublication
isbn_text = 020a
isbn_display = 020a
oclc_text = custom, getOclcNum
        
call_number_facet = custom, getCallNumberPrefix
date_indexed_facet = index_date
source_facet = "Library Catalog"
subject_era_facet = 650d:650y:651y:655y
topic_form_genre_facet = 650a:650b:650x:655a
subject_geographic_facet = 650c:650z:651a:651x:651z:655z
broad_format_facet = 000[6]:007[0], format_maps.properties(broad_format), first
format_facet = 999t, format_maps.properties(format)
language_facet = 008[35-37]:041a:041d, language_map.properties
location_facet = 999k:999l, location_map.properties
library_facet = 999m, library_map.properties
instrument_facet = 048a[0-1], instrument_map.properties
recording_type_facet = 000[6], (map.recording_type)
recordings_and_scores_facet = custom, getRecordingAndScore
recording_format_facet = custom, getRecordingFormat, format_maps.properties(recording_format)
music_catagory_facet = 999a[0-1]:999a[0], music_maps.properties(music_catagory), first
ports_of_call_facet = 650c:650z:651a:651x:651z:655z, semester_at_sea.properties(port_of_call)
guide_book_facet = 651v, (pattern_map.guide_book)
composition_era_facet = era, music_maps.properties(composition_era)
}}}

If any duplicates are found in the index specification file, the last entry that occurs will be used, and previous entries will be silently ignored. Also note that due to how java handles properties files, the order in which the entries occur is unimportant, if there is some reason you need the index fields to be added to the Solr index in a certain order, you will need to customize this program.

The values that are defined for the index field entries consist of 1 to 3 fields separated by commas.  The first field specifies either the MARC field or fields that the index entry should be extracted from or it indicates that the field is a special case.  These special cases are described below:

The simplest case is when a quoted string appears after the equals sign. Everything that appears in the quotes is taken verbatim and added to the Solr index as is. So in the above example, every record added to the Solr index by this program will have a value of {{{“Library Catalog”}}} stored for the index field named {{{source_facet}}}.  This can be useful when data from several different sources is being added to the same Solr index, to allow searchers to narrow their search to data from one or another of the sources.


If the value after the equals sign is {{{FullRecordAsMARC}}}, {{{FullRecordAsXML}}}, {{{date}}}, {{{index_date}}}, or {{{custom}}} the following actions will be taken:

 * *FullRecordAsMARC:* specifies that the entire MARC record should be added in the standard binary form (ISO 2709).

 * *FullRecordAsXML:* specifies that the entire MARC record should be added encoded using the MARCXML standard.

 * *!FullRecordAsText:* specifies that the entire MARC record should be translated to a readable format, and stored, (with {{{<br/>}}} tags being inserted in place of newline characters.

 * *date:* extracts the 260 c subfield from the MARC record, and then extracts a four digit year from that subfield.

 * *index_date:* returns a value for the date and time of when the record was indexed.

 * *custom:* specifies that a custom java routine is to be invoked to extract the value for this field. Examples of when custom might be necessary are:  
   # Create an index entry based on the value in field X, but only if a certain value appears in field Y.  
   # Create an index entry that consists of the first characters from a given field, but only as many characters as are letters.  
   # Create an index entry based on a portion of a given field, but only if the remaining portion of that field contains a certain value.   
 Many custom indexing functions are provided in the !SolrMarc code; you can also write your own. Details of how to define a custom indexing routine are explained later in this document.

If the value after the equals sign is not one of these special cases entries, it is assumed to be a list of MARC fields from which to extract the data to use for the index field.

<br />

== Specifying Which Fields and Subfields to Use ==

The syntax for specifying what fields/subfields (or what portion of a field or subfield) is to be looked-at to create the Solr index field(s) consists of one or more field specifications separated by colons (:).  

A field specification consists of a three-digit string (000 – 999) optionally followed by characters indicating which subfields and/or bytes to use. 

 * *no subfields specified*, e.g {{{100}}} - all subfields of the specific Marc field, in order of occurrence in the Marc record, will be concatenated into a single value.  Each occurrence of the Marc field will create separate instance of the Solr field in the Solr document. 

 * *single letter after the field*, e.g. {{{041a}}} - for each occurrence of the Marc field, each occurrence of the subfield will create a Solr field instance of the contents of the subfield.  Note: if you want the subfield values to be concatenated, repeat the letter, e.g. {{{650aa}}}

 * *multiple letters after the field*, e.g. {{{100abcdq}}} - for each occurrence of the field, all the indicated subfields, in order of occurrence in the Marc record, will be concatenated into a single value.  Each occurrence of the Marc field will create separate instance of the Solr field in the Solr document. 

 * *square brackets containing digit pattern* for a fixed length field (i.e. leader, 001-009), , e.g. {{{008[35-37]}}} or {{{000[5]}}} - the digits in the brackets indicate the characters to be used as a value. The counting is 0-based: the first byte in the fixed field is 0.  {{{008[35-37]}}} will return the three character sequence at bytes 35,36,37 in the 008 field.  Each instance of the Marc field (e.g. for an 007, which is repeatable) will create a separate instance of the Solr field in the Solr document.
   NOTE: as of 2009-08-27, numbers will grab bytes for all fields, not just fixed fields.  This is intended to change to the behavior described.
 
 * *square brackets indicating a regular expression describing subfields* for a variable length field, , e.g. {{{110[a-z]}}} or {{{243[a-gk-s]}}}. for each occurrence of the field, all the indicated subfields, in order of occurrence in the Marc record, will be concatenated into a single value.  Each occurrence of the Marc field will create separate instance of the Solr field in the Solr document. 
   NOTE: as of 2009-08-27, the distinction is letters vs. digits.  This intended to change to allow numeric subfields in the future.

TODO: document specification of separator.

There are additional ways to generate Solr fields from your MARC data explained below.

<br />
=== Example Field Specifications ===

{{{full_title_display = 245}}}

  for _each_ MARC 245 field, concatenate _all subfield_ values, separated by a space, then add a field named full_title_display to the Solr document with the concatenated value.  Note that there is a single Solr field occurrence for each specified MARC _field_.

{{{brief_title_display = 245a}}} 

  for _each_ subfield _a_ in each of the 245 fields in the MARC record, add a field to the Solr document named brief_title_display, with the value in from the MARC 245 subfield a.  Note that there is a single Solr field occurrence for each specified MARC _subfield_ in the MARC _field_.
Aside:  since the MARC specification states that there can only be a single 245 field and only a single subfield _a_, the results of this specification will be identical to what they would be if the field specification was{{{brief_title_display = 245a, first}}}. 

{{{author_text = 100a:110a:111a:130a}}}

  for _each_ 1) subfield a in each of the 100 fields 2) subfield a in each of the 110 fields 3) each subfield a in each of the 111 fields 4) subfield a in each of the 130 fields in the MARC record, add a field to the Solr document named author_text with the value from the MARC subfield specified.  Note that each of these values is added as a separate Solr field in the Solr document.  

{{{author_addl_t = 700abcegqu:710abcdegnu:711acdegjnqu}}}

  for _each_ occurrence of a specified MARC field (700, 710 and 711), concatenate the values of the specified subfields, with a space separator, then add a field named author_addl_t to the Solr document with the concatenated value.  Note that there is a single Solr field occurrence for each specified MARC _field_.

{{{material_type_display = 300aa}}}

  for _each_ MARC 300 field, concatenate all subfield _a_ values, separated by a space, then add a field named material_type_display to the Solr document with the concatenated value.  Note that there is a single Solr field occurrence for each MARC _field_, not each MARC _subfield_.  

{{{title_added_entry_t = 700[gk-pr-t]:710[fgk-t]:711fgklnpst:730[a-gk-t]:740anp}}}

  The subfields specified here are regular expressions.  This field spec is equivalent to
 {{{title_added_entry_t = 700gklmnoprst:710fgklmnopqrst:711fgklnpst:730abcdefgklmnopqrst]:740anp}}}

{{{language_facet = 008[35-37]:041a:041d, language_map.properties}}}

  This field specification states that characters 35 through 37 should be selected from _each_ 008 control field of the MARC record (which is where a three-letter encoding of the primary language of a bibliographic work is found.)  Additionally, _each_ occurrence of the _a_ and _d_ subfields of all 041 fields in the MARC record become individual Solr fields named language_facet in the Solr document.

  Note that a second parameter is present on the field specification entry: {{{language_map.properties}}}.  If this optional parameter is present, once the set of strings is created for all of the fields and subfields specified in the first parameter, the entire set is translated using the translation map that is defined in the separate property file named {{{language_map.properties}}}, (which maps the three-letter abbreviations for languages to the full name of that language; Hence "eng" becomes "English," "fre" becomes "French," "chp" becomes "Chipewyan," and "peo" becomes the ever-popular "Old Persian (ca. 600-400 B.C.)."  The details of how to define a translation map is covered in the next section.

  (Naomi doesn't believe this is true anymore - 2009-08-28:  you CANNOT specify 041ad in the field specification, each colon separated item can only reference a single field and subfield.)  

{{{broad_format_facet = 000[6]:007[0], format_maps.properties(broad_format), first}}}

  This field specification states that the value of character 6 (counting from 0) of field 000 (which stands for the leader of the MARC record) and character 0 of field 007 are to be extracted.  Both of these values are to then be translated using the translation map that is defined in the separate property file named {{{format_maps.properties}}}, by loading all the entries there that start with the string {{{broad_format}}}. The first translated value is to be used as the value for the Solr index entry.  

Or to put it more succinctly, look up character 6 of the 000 field in the map {{{broad_format}}}, if the map contains a mapping for that character, use that value; otherwise, look up character 0 of the 007 field in the map {{{broad_format}}}, if the map contains a mapping for that character use that value. If neither extracted value matches an entry in the translation map, check to see whether the map defines a default value, if so use that default value, otherwise leave the {{{broad_format_facet}}} index entry unassigned.

<br />
== Mapping "Raw" Values to New Values ==

The process of creating a translation map to translate from the cryptic entries found in the MARC record to more human-readable strings to make searching and faceting more useful to the end-user of the system, is fairly straightforward.  The first thing that you must do is add a second parameter on the field specification entry in the properties file, as shown in the last two of the examples shown above. This parameter specifies either the name of a separate property file that contains the map or the name of a separate property file plus the name of the property key prefix that should be looked for in that file.  For the last example above, a map named {{{broad_format}}} is referenced in the properties file {{{format_maps.properties}}}. Those entries in that file that start with the string {{{broad_format}}} will be used to define the map. The definition for that map:

{{{
broad_format.v = Video
broad_format.a = Book
broad_format.t = Book
broad_format.m = Computer File
broad_format.c = Musical Score
broad_format.d = Musical Score
broad_format.j = Musical Recording
broad_format.i = Non-musical Recording
broad_format = Unknown
}}}

Note that {{{Unknown}}} is the default value for the translation map.

Each line defining a translation starts with the name of the map, followed by a period, followed by the string that is to be replaced.  Next there must be an equals sign, followed by the string that should be used to create the replacement.  Note that it is possible to have several different strings be mapped to the same result (as shown for _Book_ or _Musical Score_), but it is not possible to have the same string to map to two different results.  If, for instance, in this specific example, which looks at position 6 from the MARC leader, and position 0 of field 007, if you decided that you wanted to include a mapping for the character _r_ in position 6 of the leader to _Three-dimensional artifact_ and also include a mapping for character _r_ of position 0 of field 007 to _Remote-sensing image_, you could not accomplish this using a field specification and a translation.  Instead you would have to create a custom indexing function.

Note also, that if no mapping is present for a given input, then no value will be entered for that particular index entry in the Solr index record.  This fact can be exploited in conjunction with the {{{first}}} field specification command as is shown in the following example:

 {{{music_catagory_facet = 999a[0-1]:999a[0], music_maps.properties(music_catagory), first}}}
	
 {{{
music_catagory.ML = Music Literature
music_catagory.MT = Music Theory
music_catagory.M2 = Monuments of Music
music_catagory.M3 = Composers' Collected Works
music_catagory.M = Printed Music
}}}

In this example, the first two characters of the 999a subfield are extracted, if these two characters are _ML_, _MT_, _M2_ or _M3_, then the translation map will return the value corresponding to those values.  If the value doesn’t match one of those four strings, then the translation map will return null, and the next step in the specification will be processed; it will take only the first character of the ‘999a’ subfield, and pass that to the translation map, which then can check against the single letter _M_ using the fifth map entry.  If the value matches, then _Printed Music_ will be used for the Solr index field entry, otherwise no value will be used for the {{{music_catagory_facet}}} field of the Solr index record.

Lastly, note that the process of winnowing out duplicate entries takes place both before the translation map is applied, and again while collecting the results from applying the translation map. So if the following map were applied:

 {{{
recording_format.MUSIC-CD = CD
recording_format.RSRV-CD = CD
recording_format.AUDIO-CD = CD
      
recording_format.AUDIO-CASS = Cassette
recording_format.MUSIC-CASS = Cassette
recording_format.RSRV-CASS = Cassette
recording_format.RSRV-AUD = Cassette
recording_format.RSRV-CAS2D = Cassette
      
recording_format.DVD = DVD 
recording_format.HS-VDVD = DVD 
recording_format.HS-VDVD3 = DVD 
recording_format.RSRV-VDVD = DVD 
      
recording_format.LP = LP
recording_format.IVY-LP = LP
recording_format.MUSIC-LP = LP
recording_format.OPENREEL = Open Reel Tape
      
recording_format.VIDEO-CASS = VHS
recording_format.RSRV-VCASS = VHS

recording_format.VIDEO-DISC = Video Disc
recording_format.RSRV-VDISC = Video Disc
}}}

and the set of strings gathered for the item consisted of  {{{{AUDIO-CASS, MUSIC-CASS}}}} the final returned result would be {{{{Cassette}}}}.

In the case where you want to define only a single translation map in a properties file, which might be the case for large translation maps, you can specify only the name of the properties file on the index field specification line as shown below:

 {{{instrument_facet = 048a[0-1], instrument_map.properties}}}

In this case all of the entries that occur in that file will be used to define the translation map, and there is no need to prefix the property keys with a common string, so that the instrument map would be defined as follows:

 {{{
ba = Horn
bb = Trumpet
bc = Cornet
bd = Trombone
be = Tuba
bf = Baritone horn
bn = Brass, Unspecified
bu = Brass, Unknown
by = Brass, Ethnic
bz = Brass, Other
ca = Choruses, Mixed
cb = Chorus, Women's
cc = Choruses, Men's
cd = Choruses, Children's
cn = Choruses, Unspecified
cu = Chorus, Unknown
cy = Choruses, Ethnic
ea = Synthesizer
eb = Electronic Tape
ec = Computer
ed = Ondes Martinot
en = Electronic, Unspecified
eu = Electronic, Unknown
}}}

This makes the creation and maintaining of translation map properties files much easier to understand.

<br />

=== Defining a Pattern-Based Translation Map ===

The previous section described how to define a translation map for a field.  However, one limitation of it is that it can only map from a fixed, pre-specified set of values.  If the value in the field doesn’t exactly match one of the translation keys, that value will not be mapped to any other value, and usually would then be discarded.  

Sometimes you may want to look for a pattern of characters somewhere in the input field, and if that pattern occurs, then output some value to the index field.  To specify this in the field specification entry, specify the name of the translation map as described above:

 {{{ports_facet = 650c:650z:651a:651x:651z:655z, semester_at_sea.properties(port)}}}

Then define the translation map like this:

 {{{
port.pattern_0 = Nassau.*Bahamas=>Nassau
port.pattern_1 = Salvador.*Brazil=>Salvador
port.pattern_2 = Walvis Bay.*Namibia=>Walvis Bay
port.pattern_3 = Cape Town.*South Africa=>Cape Town
port.pattern_4 = Chennai.*India=>Chennai
port.pattern_5 = Penang.*Malaysia=>Penang
port.pattern_6 = Ho Chi Minh City.*Vietnam=>Ho Chi Minh City
port.pattern_7 = Hong Kong=>Hong Kong
port.pattern_8 = Shanghai.*China=>Shanghai
port.pattern_9 = Kobe.*Japan=>Kobe
port.pattern_10 = Yokohama.*Japan=>Yokohama
port.pattern_11 = Puntarenas.*Costa Rica=>Puntarenas
port.pattern_12 = Bombay.*India=>Chennai
port.pattern_13 = Namibia=>Namibia
port.pattern_14 = South Africa=>South Africa
port.pattern_15 = India=>India
port.pattern_16 = Malaysia=>Malaysia
port.pattern_17 = Vietnam=>Vietnam
port.pattern_18 = China=>China
port.pattern_19 = Japan=>Japan
port.pattern_20 = Costa Rica=>Costa Rica
port.pattern_21 = Bahamas=>Bahamas
port.pattern_22 = Brazil=>Brazil
}}}

Then for every field that is extracted from a given MARC record will be matched against all of the patterns specified in the map.  Note that these entries must start with {{{(map_identifier).pattern_0}}} and proceed sequentially from there.  When using multiple translation maps, each map identifier ("port" in the example above) must be unique.  The value of the pattern is then split at the _=>_ with the portion before the arrow being used as a regular expression, and if that regular expression matches anywhere inside any of the fields extracted from the MARC record, the string that occurs after the arrow will be added to the index record.

In this example if a single field extracted from the MARC record contained _Chennai_, followed eventually by India, the value Chennai would be added to the index.  If that same field also contained _Penang_ followed by _Malaysia_, the value _Penang_ would be added to the index also.  Notice that for the last entries in the map above, the pattern that is looked-for is a simple string.  So based on {{{pattern_19}}} above if one of the fields extracted from the MARC record contains the word Japan, then the word Japan will be added to the index.

Another way of using the pattern-based translation map feature is to trim out a portion of the original string, using the regular expression grouping characters (   and   )  and the $1 syntax for the replacement string.  For example, suppose your records have several 035 fields, and that some of these field contain OCLC numbers, which are indicated in the field by having a prefix of (OCLC) before the number to use as shown in the following example record:

 {{{
LEADER 00873pam a2200277 a 4500
001 u17922
008 831011s1984    njua          00110 eng
010   $a   83022049
020   $a0135959195 (pbk.)
035   $a(Sirsi) l83022049
035   $a(OCLC)10072685
039 0 $a2$b3$c3$d3$e3
040   $aDLC$cDLC$dVA@
049   $aVA@&
050 0 $aZ52.4$b.G34 1984
082 0 $a652$219
090   $aZ52.4$b.G34 1984$mVA@&$qGRAD BUS.
100 1 $aGalloway, Dianne.
245 10$aLearning to talk word processing /$cDianne Galloway.
260   $aEnglewood Cliffs, N.J. :$bPrentice-Hall,$cc1984.
300   $aviii, 119 p. :$bill. ;$c23 cm.
490 0 $aThe Modern office series
500   $aIncludes index.
596   $a13
650  0$aWord processing.
}}}

For this example if you wanted to select only the number portion of 035 lines that were OCLC numbers you could use the following index specification:

 {{{oclc_text = 035a, (pattern_map.oclc_num)}}}

and then use the following pattern-based translation map:

 {{{pattern_map.oclc_num.pattern_0 = \\(OCLC\\)(.*)=>$1}}}

which will discard the first 035 field from above, and then map the second field to the value _10072685_.

Similarly, if you want to trim off everything following the initial letters of an LC call number, you could use the following pattern map:

 {{{pattern_map.call_num.pattern_0 = ([A-Za-z]*).*=>$1}}}

<br />

== Custom Indexing Routines ==

If special processing is required for a given Solr index field that cannot be handled by the any of the above indexing specification commands, you can reference a custom-created indexing function, by specifying {{{custom}}} as the second value in the field specification entry as shown in the example below:

 {{{recordings_and_scores_facet = custom, getRecordingAndScore}}}

In this case the value after _custom_ specifies the name of a java method to handle this particular bit of indexing.

<br />

=== Pre-Defined Custom Indexing Routines ===

Because there are many "special processing" indexing tasks that commonly occur when indexing MARC records, there are several pre-defined "custom" indexing routines that can be referenced from your index specification file without requiring you to write any java code. These pre-defined indexing routines are listed below with a short description of what each of them does.

*getAllAlphaExcept(fieldSpec)*:
  For _each_ occurrence of a MARC field in the !fieldSpec list, concatenate the alphabetic subfield contents _except_ the ones specified, using a space separator.

  {{{geographic_search = custom, getAllAlphaExcept(651vxyz:691vxyz)}}}

   For each MARC 651 and 691 field, concatenate all the subfields from a to u, in order of occurrence, and add a field to the Solr document named geographic_search with the concatenated value.
 Currently, the !fieldSpec may be one or more letters: it will not recognize regular expressions and it will include all alpha if there are no subfields specified.

*getAllAlphaSubfields(fieldSpec)*:
   For _each_ occurrence of a MARC field in the !fieldSpec list, concatenate all alphabetic subfield contents, using a space separator.

  {{{title_full_display = custom, getAllAlphaSubfields(245)}}}
 
    For each MARC 245 field, concatenate all alphabetic subfield values, then add a field named full_title_display to the Solr document with the concatenated value.  

*getAllAlphaSubfields(fieldSpec, first|join|all)*:
 As above, but using "first" "join" or "all" to indicate handling of multiple occurrences of field values

  {{{title_uniform_display = custom, getAllAlphaSubfields(130:240, first)}}}

    If the MARC record has a 130 field, concatenate all alphabetic subfield values, then add a field named title_uniform_display to the Solr document with the concatenated value.  If there is no 130 field in the MARC record, look for a 240 field, concatenate all alphabetic subfield values, then add a field named title_uniform_display to the Solr document with the concatenated value.
   "join" - all the values indicated by the !fieldSpec will be concatenated into a single value.
   "all" - each value indicated by the fieldSpec will be a separate field in the Solr document.

*getAllSubfields(fieldSpec, separator)*:
 operates similarly to the standard indexing specification, but it provides a little more power. If your author field specification was as follows:

 {{{author_person_search = 100abcdefghijklmnopqrstuvwxyz:700abcdefghijklmnopqrstuvwxyz}}}

 You could shorten it and make it easier to read thusly:

 {{{author_person_search = custom, getAllSubfields(100:700,  " ")}}} 

 Or, if you needed to be sure that only alphabetic subfields would be included:

 {{{ author_person_search = custom, getAllSubfields(100[a-z]:700[a-z],  " ") }}}

*getAllSearchableFields(lowerBound, upperBound)*: 
  retrieves the contents of all fields/subfields where the field tag is greater or equal to the lower bound parameter, and less than the upper bound parameter. This is useful for grabbing all data from all fields to put into a default, catch-all search field.  Usually you would want to specify 100 as the lower bound and 900 as the upper bound thusly:

 {{{marc_text = custom, getAllSearchableFields(100, 900)}}}

*getEra*:
 gets the era field values from 045a, which contains a coded time period (see http://www.loc.gov/marc/bibliographic/bd045.html ).  When combined with a translation map provided by the !SolrMarc code, values are end-user friendly.  For example:
  {{{composition_era_facet = custom, getEra, composition_era_map.properties}}}

*getFullTextUrls*:
 gets the URLs for the full text of a resource described by the MARC record.  
 For each MARC 856 field, 
   if the second indicator is 0, _each_ subfield u is added to the Solr document.  
   if the second indicator is 2, no value is added to the Solr document.
   otherwise, subfields 3 and z are examined.  If _none_ of the following text is present, then subfield u is added to the Solr document
    * abstract
    * description
    * sample text
    * table of contents
  {{{url_fulltext = custom, getFullTextUrls}}}

*getLinkedField(fieldSpec)*:
 allows you to retrieve the linked original-language version of a given field, if such an original language version exists, a 880 field. For example, items that are originally in Chinese, Japanese, Korean, Arabic, Hebrew, or Russian, the cataloger usually transliterates the title and author fields from their original language into a latin-alphabet, phonic representation of the original title or author string, placing the original title or author in a 880 field with a tag indicating that that entry is linked to the transliterated title or author string in the main body of the MARC record.  For the example record below, if you wanted to extract the original version of the title, you could use the following specification: 

 {{{linked_title_display = custom, getLinkedField(245abc)}}}

 Which would extract  {{{"紫禁城宮殿 /于倬雲主編 ; [攝影高志強, 胡錘]."}}} for the field {{{linked_title_display}}}.

 {{{
LEADER 01584cam a2200397 a 4500
001 u2103
003 SIRSI
005 20030106152914.0
008 840523s1982    cc af    b    000 0 chi d
010   $a   83111383
020   $a9620750012
035   $a(Sirsi) o10765514
040   $aViBlbV$cViBlbV$dOrU$dCLASIA$dMH-HY$dNcU$dOCoLC
043   $aa-cc-pe
066   $c$1
082 00$a725/.17/0951156
245 00$6880-01$aZi jin cheng gong dian /$cYu Zhuoyun zhu bian ; 
       [she ying Gao Zhiqiang, Hu Chui].
260   $6880-02$aXianggang :$bShang wu yin shu guan Xianggang fen guan, $c1982.
300   $a331 p. :$bchiefly ill. (some col.) ;$c37 cm.
500   $6880-03$aIncludes 1 portfolio (2 leaves of plates): Zi jin cheng gong
       dian yu xi yuan qian.
500   $aErrata slip inserted.
504   $aBibliography: p. 328.
596   $a4
610 20$aForbidden City (Beijing, China)
650  0$aPalaces$zChina$zBeijing.
700 1 $6880-05$aYu, Zhuoyun.
700 1 $6880-06$aGao, Zhiqiang.
700 1 $6880-07$aHu, Chui.
880 00$6245-01/$1$a紫禁城宮殿 /$c于倬雲主編 ; [攝影高志強, 胡錘].
880   $6260-02/$1$a香港 :$b商務印書館香港分館,$c1982.
880   $6500-03/$1$aIncludes 1 portfolio (2 leaves of plates): 紫禁城宮殿玉璽原鈐.
880 1 $6700-05/$1$a于倬雲.
880 1 $6700-06/$1$a高志強.
880 1 $6700-07/$1$a胡錘.
987   $c20010620$dc$e06.12.2001$f500-a /
}}}

 The !fieldSpec will be treated in the standard fashion, except _any_ !fieldSpec containing square brackets, such as {{{245[a-c7]}}} will be treated as a regular expression of desired subfields. 

*getLinkedFieldCombined(fieldSpec)*: 
 operates similarly to {{{getLinkedField}}}, including not only the original language version of the specified field, but also the cataloger created transliterated version. This would be done for fields used for searching so that a user could search by title by specifying either the transliterated version or the original language version. For the example record, if you wanted to extract the original and the transliterated versions of the title, you could use the following specification: 

 {{{title_text = custom, getLinkedFieldCombined(245a)}}}

 Which would extract {{{“紫禁城宮殿”}}} and “Zi jin cheng gong dian” for the field {{{title_text}}}.

 The !fieldSpec will be treated in the standard fashion, except _any_ !fieldSpec containing square brackets, such as {{{245[a-c7]}}} will be treated as a regular expression of desired subfields. 

*getSortableAuthor*: 
 A sortable author value is added to the Solr document.  This value is a string containing the concatenated values of:
  1. the main entry author, if there is one (fields 100, 110 or 111)
  2. the main entry uniform title (240), if there is one - not including non-filing chars as noted in 2nd indicator of the 240
  followed by
  3. the 245 title, not including non-filing chars as noted in ind 2 of the 245

  NOTE: in your Solr schema.xml file, you'll want to ensure this field as the desired properties.  Do you want sorting to be case sensitive?  Do you want it to normalize diacritics?  Do you want it to remove preceding or trailing whitespace?  You may want to use the alphaOnlySort field type provided in the standard Solr schema.xml file.

*getSortableTitle*: 
 retrieves the value from the 245a subfield, and then strips off any "non-sorting characters" at the beginning of the string, according to the MARC records indicators at the head of the field.

  NOTE: in your Solr schema.xml file, you'll want to ensure this field as the desired properties.  Do you want sorting to be case sensitive?  Do you want it to normalize diacritics?  Do you want it to remove preceding or trailing whitespace?  You may want to use the alphaOnlySort field type provided in the standard Solr schema.xml file.

*getSupplUrls*:
 gets the URLs for the full text of a resource described by the MARC record.  
 For each MARC 856 field, 
   if the second indicator is 0, no value is added to the Solr document.
   if the second indicator is 2, _each_ subfield u is added to the Solr document.  
   otherwise, subfields 3 and z are examined.  If _any_ of the following text is present, then subfield u is added to the Solr document
    * abstract
    * description
    * sample text
    * table of contents
  {{{url_suppl = custom, getSupplUrls}}}

*getTitle*:
 Create a Solr field containing the 245a (and 245b, if it exists, concatenated with a space between the two subfield values), with trailing punctuation removed.  Removed punctuation includes space, comma, slash, semicolon, colon, trailing period if it is preceded by at least three letters, and single square bracket characters if they are the start and/or end chars of the cleaned string

*removeTrailingPunct(fieldSpec)*:
 creates a list of entries extracted from the fields/subfields specified by the {{{fieldSpec}}} parameter, and then post-processes all of the entries in the list to remove any trailing punctuation from the entries.  So if you were defining a index entry like this

 {{{title_display = 245a}}}

 but you were unhappy with the colons, commas, and slashes which appear on the entries that end up being stored in the index, you could change the index specification to be:

 {{{title_display = custom, removeTrailingPunct(245a)}}}

 The specific characters that are removed are: space, comma, slash, semicolon, colon, trailing period if it is preceded by at least three letters, and single square bracket characters if they are the start and/or end chars of the cleaned string.

 The !fieldSpec will be treated in the standard fashion, except _any_ !fieldSpec containing square brackets, such as {{{245[a-c7]}}} will be treated as a regular expression of desired subfields.

=== Creating Custom Indexing Routines ===

If you are still unable to accomplish what you want to do using any of the provided indexing techniques, there is the option to create your own custom indexing functions.

Steps to create a custom indexing function:

 # Create a java source file defining a class, specifying that the new class extends the class SolrIndexer.
 # Define a Constructor for that new class that accepts a String naming a properties file, and an array of strings specifying the paths where the property files can be found.
 # Define a function to correspond to every as-of-yet-undefined custom indexing function that appears in your index specification file.

For example, if you include the following index specification:

 {{{recordings_and_scores_facet = custom, getRecordingAndScore}}}

You will need to define a custom function in your java source file named {{{getRecordingAndScore}}} as show below:

 {{{
import org.solrmarc.index.SolrIndexer;

public class BlacklightIndexer extends SolrIndexer
{

public BlacklightIndexer(String propertiesMapFile, String propertyPaths[])
    {
        super(propertiesMapFile, propertyPaths);
    }

    public Set<String> getRecordingAndScore(Record record)
    {
        Set<String> result = new LinkedHashSet<String>();
        String leader = record.getLeader().toString();
        String leaderChar = leader.substring(6, 7).toUpperCase();
                
        if(leaderChar.equals("C") || leaderChar.equals("D"))
        {
            result.add("Scores");
            result.add("Recordings and/or Scores");
        }
        
        if(leaderChar.equals("J"))
        {
            result.add("Recordings");
            result.add("Recordings and/or Scores");
        }
        
        return result;
    }

//  other custom indexing functions go here

}
}}}

This example looks in position 6 of the leader of the MARC record, and if the value there is a _C_ or a _D_ it will return a set containing two values to be added to the index entry named {{{recordings_and_scores_facet}}} in the Solr index record, and if the value in that character is _J_ it will return a set containing two other values.  If any other value occurs in that location, this routine will return an empty set, and no values will be added for this index entry in the Solr index record.  Because this routine must return two items based on a single input item, there is no way that it could be handled via the standard index specification syntax.  

The next step to make sure that your main properties file lists that name of your new custom class as the value for the {{{solr.indexer}}} property, as shown here:

 {{{solr.indexer = BlacklightIndexer}}}

Note that if you need to define more than one custom indexing function, they *must* all be defined in the same java file.   Also note that all custom indexing functions must be defined as *public*, must return either a {{{String}}} or a {{{Set<String>}}}, and must accept a parameter consisting of the {{{Record}}} being operated upon, followed by zero or more String parameters that can be used for greater control over the operation of the custom function. 

<br />
==== Example Custom Indexing Routines ====

*_Create an index entry based on the value in field X, but only if a certain value appears in field Y._*  

 {{{
    public Set<String> getRecordingFormat(Record record)
    {
        String mapName = loadTranslationMap(null, "recording_format_map.properties");
        Set<String> result = new LinkedHashSet<String>();
        String leader = record.getLeader().toString();
        String leaderChar = leader.substring(6, 7).toUpperCase();
        Set<String> titleH = new LinkedHashSet<String>();
        addSubfieldDataToSet(record, titleH, "245", "h");       
                
        if(leaderChar.equals("J") || leaderChar.equals("I") || 
                (setItemContains(titleH, "videorecording")))
        {
            Set<String> form = new LinkedHashSet<String>();
            addSubfieldDataToSet(record, form, "999", "t");
            Set<String> labels = Utils.remap(form, findMap(mapName"));
            return(labels);
        }
        return(result);
    }
}}}

Note that the above routine invokes a translation map before it returns its results, you can either handle it this way in a custom indexing routine, or return the values unmapped, and include a third parameter on the field specification entry as described above for the other index specifications.  

<br />
*_Create an index entry that consists of the first characters from a given field, but only as many characters as are letters._*  

 {{{
      public String getCallNumberPrefix(Record record)
    {
        String val = getFirstFieldVal(record, "999a:090a:050a");
        if (val == null || val.length() == 0) return(null);
        String vals[] = val.split("[^A-Za-z]+", 2);
        if (vals.length == 0 || vals[0] == null || vals[0].length() == 0)
     return(null);
        return(vals[0]);
    }
}}}

This custom function returns the beginning portion of the call number (extracted from either 999a or 090a or 050a  field) but only up until the first character that is not a letter.

<br />
*_Create an index entry based on a portion of a given field, but only if the remaining portion of that field contains a certain value._*   

 {{{
    public String getOclcNum(Record record)
    {
        Set<String> set = getFieldList(record, "035a");
        if (set.isEmpty())  return(null);
        Iterator iter = set.iterator();
        while (iter.hasNext())
        {
            String value = (String)iter.next();
            if (value.contains("(OCoLC)"))  
            {
                value = value.replaceAll("\\(OCoLC\\)", "");
                return(value);
            }
        }
        return null;
    }
}}}

This routine gathers all 035a fields in the MARC record, and scans through them looking for one labeled with the string _(OCoLC)_ indicating that it is an OCLC number, the routine strips out the string _(OCoLC)_ and returns the remaining string which is the OCLC number of the item in the MARC record. If no 035a field containing that label is found, no OCLC number is added to the Solr index record for this item.

== Custom Indexing Scripts ==

A new feature that was recently added to SolrMarc is that ability to have custom indexing scripts in addition to custom indexing routines.  The difference being that the scripts are interpreted at run-time, therefore no compiling is necessary.  So custom indexing scripts are a way to extend !SolrMarc even if you started from a Pre-built binary release, and are unable to compile custom indexing routines.  The syntax of the language [http://www.beanshell.org BeanShell] for creating a custom indexing script is _very_ similar to java.  If you obtain the java source code for a custom indexing routine from someone, and want to implement that routine in a !BeanShell script, it usually takes only about 5 minutes to manually convert.  

The syntax that you need to use in your index.properties file to reference a custom indexing script is very similar to was described above for referencing a custom indexing routine.  You can reference a custom-created indexing script, by specifying {{{script}}} as the second value in the field specification entry as shown in the example below:

 {{{callnumber = script(callnumber.bsh), getFullCallNumber}}}

In this case the value in parentheses after _script_ specifies the name the !BeanShell script that contains the custom scripted indexing method that you want to use, and the value following the comma is the name of the scripted method to invoke to handle this particular bit of indexing.

<br />

=== Creating Custom Indexing Scripts===

Note before you go to the work to create a new custom indexing script, see the above section on [#Pre-Defined_Custom_Indexing_Routines Pre-Defined Custom Indexing Routines], because there may already be a custom routine included in SolrMarc to accomplish what you want.  If you are still unable to accomplish what you want to do using any of the provided indexing techniques, there is the option to create your own custom indexing functions.

Steps to create a custom indexing function:

 # Create a !BeanShell script file in the index_scripts directory, defining one or more scripted custom indexing methods.
 # Add {{{import}}} statements to the top of the !BeanShell script file referencing all of the non-builtin-java classes your scripts reference, such as classes from SolrMarc or from marc4j.
 # Add a script-wide variable declaration of a variable named indexer: {{{org.solrmarc.index.SolrIndexer indexer = null;}}}  Note that the SolrIndexer code will set this value before script methods are called.

For example, if you include the following index specification:

 {{{callnumber = script(callnumber.bsh), getFullCallNumber}}}

You will need to define a script named callnumber.bsh in the index_scripts directory as show below:

 {{{
import org.marc4j.marc.Record;

// define the base level indexer so that its methods can be called from the script.
// note that the SolrIndexer code will set this value before the script methods are called.
org.solrmarc.index.SolrIndexer indexer = null;

/**
 * Extract the call number label from a record
 * @param record
 * @return Call number label
 */
public String getFullCallNumber(Record record) {

    String val = indexer.getFirstFieldVal(record, "099ab:090ab:050ab");

    if (val != null) 
    {
        return val.toUpperCase().replaceAll(" ", "");
    } 
    else 
    {
        return val;
    }
}

}}}

This example gets the first occurance of field 099, 090 or 050 and extract the a and b subfields from that field.  It then shifts the returned string to all uppercase letters and strips out spaces.

Note that if you want to define more than one scripted custom indexing method, they *can* all be defined in the same !BeanShell custom indexing script file, or they can be separated into several different !BeanShell files.  Also note that all scripted custom indexing methods must return either a {{{String}}} or a {{{Set}}}, and must accept a parameter consisting of the {{{Record}}} being operated upon, followed by zero or more {{{String}}} parameters that can be used for greater control over the operation of the custom function. 

<br />
==== Example Scripted Custom Indexing Methods ====

*_Create a facet based on the Dewey Decimal numbers found in the record_*  

 {{{
import org.marc4j.marc.Record;
import org.solrmarc.tools.Utils;
import org.solrmarc.tools.CallNumUtils;

// define the base level indexer so that its methods can be called from the script.
// note that the SolrIndexer code will set this value before the script methods are called.
org.solrmarc.index.SolrIndexer indexer = null;

/**
 * returns the facet value for dewey hundreds digits, and dewey tens digits
 * @param record
 * @return Set of Strings containing facet value for dewey hundreds digits, and dewey tens digits
 */
Set getDeweyFacet(Record record, String propertiesMapName)
{
    LinkedHashSet resultSet = new LinkedHashSet();
    Set values = indexer.getFieldList(record, "082a");
    String mapName = indexer.loadTranslationMap(propertiesMapName);

    for (String dewey : values)
    {
        if (! CallNumUtils.isValidDewey(dewey))  continue;
        String hundreds = dewey.substring(0, 1) + "00";
        String tens = dewey.substring(0,2) + "0";
        String hundredsMapped = Utils.remap(hundreds, indexer.findMap(mapName), true);
        String tensMapped = Utils.remap(tens, indexer.findMap(mapName), true);
        if (hundredsMapped != null) resultSet.add(hundredsMapped);
        if (tensMapped != null) resultSet.add(tensMapped);
    }

    return resultSet;
}

}}}

Note that the above routine invokes a translation map before it returns its results (the name of the translation map to use is passed in as a parameter), you can either handle it this way in a scripted custom indexing routine, or return the values unmapped, and include a third parameter on the field specification entry as described above for the other index specifications.  In this case though since both the description of the Dewey decimal number at the hundreds level and at the tens level is desired, the mapping must be handled in this way.   Also note that if this !BeanShell scrip was written out to a file named dewey.bsh, the indexing specification to invoke this method would be:

{{{ dewey_facet = script(dewey.bsh), getDeweyFacet(callnumber_map.properties) }}}